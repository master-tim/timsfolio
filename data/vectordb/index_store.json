{"docstore/data":{"67f47b70-df09-4715-9106-0dd9c1f4e86a":{"indexId":"67f47b70-df09-4715-9106-0dd9c1f4e86a","nodesDict":{"00870887-59a2-4d9f-a18d-83f702322b12":{"id_":"00870887-59a2-4d9f-a18d-83f702322b12","metadata":{"source":"computational-design-palm-art.md","type":"blog","title":"Computational Design: Transforming Palm Lines into Art"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"3e33bc4f-960c-4c2d-b25e-63a466f37a2a","metadata":{"source":"computational-design-palm-art.md","type":"blog","title":"Computational Design: Transforming Palm Lines into Art"},"hash":"3d+3cabRyYffOajyisH4OtBzAacKDbKQUF6dWfaSIV4="},"NEXT":{"nodeId":"a609da8b-7a3b-46fd-bc66-53bb77fa79ea","metadata":{},"hash":"OdG20LWLlnzLE1Uvd3i2zvcK/OI3P2k/HDN2MUuFAhM="}},"text":"## Overview\n\nThis project explores the innovative transformation of human palm lines into visually striking tree-like patterns. By leveraging **bio-data as an artistic medium**, the work highlights the inherent beauty and uniqueness of biological information.\n\n![Computational Design Overview](/src/assets/images/projects/computational/CxD_final.002.jpeg)\n\nThe core idea revolves around capturing palm line patterns and converting them into generative visualizations using computational tools like **OpenCV** for edge detection and **L-systems** for visual representation.\n\n## Concept and Inspiration\n\n### Bio-Data as Art\n\nInspired by interactive biometric art projects like **\"Digiti Sonus,\"** the project demonstrates how personal data can be utilized to create both functional and aesthetically pleasing outputs. It emphasizes the interplay between human input and the generation of unique visual outputs.\n\n![Concept Development](/src/assets/images/projects/computational/CxD_final.003.jpeg)\n\n### The Beauty of Uniqueness\n\nEach person's palm lines are unique, much like fingerprints. This project celebrates that uniqueness by transforming these biological patterns into personalized artistic representations.\n\n## Technical Implementation\n\n### Image Processing Pipeline\n\n![Technical Pipeline](/src/assets/images/projects/computational/CxD_final.004.jpeg)\n\n#### 1. Palm Line Detection\n\n- **OpenCV** for edge detection and feature extraction\n- Image preprocessing to enhance line visibility\n- Noise reduction and filtering algorithms\n- Contour detection for palm line identification\n\n#### 2. Data Extraction\n\n- Converting detected lines into mathematical representations\n- Analyzing line patterns, angles, and intersections\n- Creating data structures for generative algorithms\n\n![Data Extraction Process](/src/assets/images/projects/computational/CxD_final.005.jpeg)\n\n#### 3. L-System Generation\n\n**L-systems** (Lindenmayer systems) are used to create tree-like structures based on the extracted palm line data:\n\n- **Axiom**: Starting point of the tree\n- **Rules**: Transformation rules based on palm line characteristics\n- **Iterations**: Recursive application of rules to create complex patterns\n\n![L-System Generation](/src/assets/images/projects/computational/CxD_final.006.jpeg)\n\n### Customization Possibilities\n\nThe system allows for various customizations:\n\n- **Branching Angles** - Adjusted based on palm line angles\n- **Branch Lengths** - Scaled according to line lengths\n- **Growth Rules** - Modified to reflect unique palm patterns\n- **Color Schemes** - Personalized based on user preferences\n\n![Customization Options](/src/assets/images/projects/computational/CxD_final.007.jpeg)\n\n## Technologies Used\n\n- **Python**: Core programming language\n- **OpenCV**: Image processing and edge detection\n- **L-Systems**: Generative algorithm for tree structures\n- **NumPy**: Numerical computations\n- **Matplotlib**: Visualization and rendering\n- **PIL/Pillow**: Image manipulation\n\n## Visual Results\n\n### Generated Tree Patterns\n\nThe project produces stunning tree-like visualizations that are:\n- **Unique** to each individual's palm lines\n- **Aesthetically pleasing** with organic, natural forms\n- **Mathematically precise** yet artistically expressive\n- **Infinitely variable** based on input parameters\n\n![Generated Patterns 1](/src/assets/images/projects/computational/CxD_final.008.jpeg)\n\n![Generated Patterns 2](/src/assets/images/projects/computational/CxD_final.009.jpeg)\n\n![Generated Patterns 3](/src/assets/images/projects/computational/CxD_final.010.jpeg)\n\n## Challenges and Limitations\n\n### Technical Challenges\n\n1. **Computational Cost**\n   - L-systems can be computationally expensive\n   - Complex patterns require significant processing time\n   - Optimization needed for real-time generation\n\n2. **Environmental Sensitivity**\n   - Camera setup requires controlled lighting\n   - Hand positioning affects detection accuracy\n   - Background interference can impact results\n\n3. **Design Limitations**\n   - Tree designs have restricted dynamism\n   - Limited variation in basic L-system rules\n   - Balance between complexity and recognizability\n\n![Challenges](/src/assets/images/projects/computational/CxD_final.011.jpeg)\n\n### Solutions and Improvements\n\n- Implemented caching for frequently used patterns\n- Created calibration tools for optimal camera setup\n- Developed adaptive algorithms for varying lighting conditions\n- Enhanced L-system rules for more diverse outputs\n\n## Applications and Impact\n\n### Potential Applications\n\n1. **Personalized Art** - Creating unique artwork for individuals\n2. **Identity Visualization** - Visual representation of biometric data\n3. **Interactive Installations** - Museum and gallery exhibitions\n4. **Educational Tools** - Teaching computational design and algorithms\n5.","textTemplate":"","endCharIdx":4820,"metadataSeparator":"\n","type":"TEXT","hash":"iz72jwIUN/ycnK/x8fVIrqN4EdTWSopUxcw46DBaM3E="},"a609da8b-7a3b-46fd-bc66-53bb77fa79ea":{"id_":"a609da8b-7a3b-46fd-bc66-53bb77fa79ea","metadata":{"source":"computational-design-palm-art.md","type":"blog","title":"Computational Design: Transforming Palm Lines into Art"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"3e33bc4f-960c-4c2d-b25e-63a466f37a2a","metadata":{"source":"computational-design-palm-art.md","type":"blog","title":"Computational Design: Transforming Palm Lines into Art"},"hash":"3d+3cabRyYffOajyisH4OtBzAacKDbKQUF6dWfaSIV4="},"PREVIOUS":{"nodeId":"00870887-59a2-4d9f-a18d-83f702322b12","metadata":{"source":"computational-design-palm-art.md","type":"blog","title":"Computational Design: Transforming Palm Lines into Art"},"hash":"0um4y7tLllLpT7xI2Moad5GEgJsmeysFeunlwM8HQXs="}},"text":"**Environmental Sensitivity**\n   - Camera setup requires controlled lighting\n   - Hand positioning affects detection accuracy\n   - Background interference can impact results\n\n3. **Design Limitations**\n   - Tree designs have restricted dynamism\n   - Limited variation in basic L-system rules\n   - Balance between complexity and recognizability\n\n![Challenges](/src/assets/images/projects/computational/CxD_final.011.jpeg)\n\n### Solutions and Improvements\n\n- Implemented caching for frequently used patterns\n- Created calibration tools for optimal camera setup\n- Developed adaptive algorithms for varying lighting conditions\n- Enhanced L-system rules for more diverse outputs\n\n## Applications and Impact\n\n### Potential Applications\n\n1. **Personalized Art** - Creating unique artwork for individuals\n2. **Identity Visualization** - Visual representation of biometric data\n3. **Interactive Installations** - Museum and gallery exhibitions\n4. **Educational Tools** - Teaching computational design and algorithms\n5. **Therapeutic Applications** - Art therapy and self-expression\n\n### Broader Implications\n\nThis project demonstrates:\n- The intersection of **biology and technology**\n- The potential of **generative art** in personal expression\n- How **computational design** can make data beautiful\n- The value of **bio-data** beyond security applications\n\n![Final Results](/src/assets/images/projects/computational/CxD_final.012.jpeg)\n\n## Future Enhancements\n\n### Technical Improvements\n\n- **Real-time Processing** - Faster generation for interactive experiences\n- **3D Visualization** - Extending trees into three-dimensional space\n- **Animation** - Creating growth animations of the tree patterns\n- **Machine Learning** - Using AI to optimize pattern generation\n\n### Creative Expansions\n\n- **Multi-Modal Input** - Combining multiple biometric data sources\n- **Interactive Installations** - Public art installations with live generation\n- **Collaborative Art** - Merging multiple palm patterns into unified designs\n- **Physical Fabrication** - 3D printing or laser cutting the generated patterns\n\n## Conclusion\n\nThis computational design project successfully demonstrates how biological information can be transformed into meaningful and beautiful artistic expressions. By merging **image processing**, **generative algorithms**, and **creative vision**, the work creates a unique bridge between the personal and the computational.\n\nThe results underline the potential for merging bio-data with creative technologies to generate meaningful and visually engaging outputs, opening new possibilities for personalized art and interactive experiences.\n\n## Research Team\n\n- **Temirlan Dzhoroev** - Software Engineer & Designer\n\n## Technologies Stack\n\n```python\n# Core Technologies\n- Python 3.x\n- OpenCV 4.x\n- NumPy\n- Matplotlib\n- L-System Implementation\n```\n\n---\n\n*This project represents the intersection of computational design, generative art, and biometric data visualization, showcasing how technology can transform personal biological information into unique artistic expressions.*","textTemplate":"","startCharIdx":3813,"endCharIdx":6887,"metadataSeparator":"\n","type":"TEXT","hash":"CExOVNF4l/jFC1fspKv9eHA3ZqOIJLJTV72oaU3xGSs="},"ffe2d24e-b70e-4ff7-b7e5-4b67d6c4c9f1":{"id_":"ffe2d24e-b70e-4ff7-b7e5-4b67d6c4c9f1","metadata":{"source":"custom-gpt-integration.md","type":"blog","title":"Building an AI Copilot with RAG & Vector Search"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"8afa3da7-d245-4b2b-8ec9-e5448ed1c733","metadata":{"source":"custom-gpt-integration.md","type":"blog","title":"Building an AI Copilot with RAG & Vector Search"},"hash":"IDXZH1EJMmSbgZdsu1Yk1Nq+nww4xcZPYYE1sNSYL14="}},"text":"## The Challenge: Context is King\n\nBuilding a generic chatbot is easy. Building an AI that understands the specific context of a user's 3D game project, their current code state, and the proprietary engine API is a different beast entirely.\n\nAt Redbrick, I architected and built a **Retrieval-Augmented Generation (RAG)** pipeline that powers our in-engine AI Copilot. This isn't just a wrapper around an APIâ€”it's a context-aware system that acts as a pair programmer for our creators.\n\n![AI Copilot Architecture](/src/assets/images/projects/gpt/gpt.png)\n\n## System Architecture\n\nThe system is built on a modern AI stack designed for low latency and high relevance.\n\n### 1. The RAG Pipeline\nInstead of relying solely on the model's training data, we inject relevant context dynamically.\n\n- **Ingestion**: We scrape and chunk our entire documentation, API references, and example projects.\n- **Embedding**: These chunks are converted into high-dimensional vector embeddings using **OpenAI's text-embedding-3-small** model.\n- **Vector Store**: We store these embeddings in a vector database (Pinecone/Weaviate) for millisecond-latency similarity search.\n\n### 2. Context Window Management\nWhen a user asks for help, we don't just send the query. We construct a sophisticated prompt:\n\n```typescript\nconst context = await vectorStore.similaritySearch(userQuery, 3);\nconst currentScript = editor.getValue();\nconst systemPrompt = `\n  You are an expert 3D Game Engine Engineer.\n  Context from documentation: ${context}\n  User's current code: ${currentScript}\n  \n  Answer the user's question based strictly on the provided context.\n`;\n```\n\nThis ensures the AI \"knows\" the engine's specific API methods, not just generic JavaScript.\n\n## Key Features\n\n### ðŸ§  Semantic Code Search\nUsers can ask \"How do I make the player jump?\" and the system performs a semantic search across our documentation to find the `player.jump()` API, even if the user didn't use the exact keyword.\n\n### âš¡ Real-time Code Generation\nThe Copilot doesn't just explain; it writes code. By feeding the current editor state into the context window, the AI generates code snippets that fit perfectly into the user's existing logic, respecting variable names and coding style.\n\n### ðŸ”„ Dynamic Context Injection\nWe dynamically inject the \"Scene Graph\" state into the prompt. If a user has an object named `Enemy_01` in their scene, the AI knows about it and can write code like `Enemy_01.move()`.\n\n## Technical Stack\n\n- **LLM**: OpenAI GPT-4 Turbo (for complex logic) & GPT-3.5 (for latency-sensitive tasks)\n- **Vector Database**: Pinecone for storing high-dimensional embeddings\n- **Orchestration**: LangChain for managing prompt templates and chains\n- **Backend**: Node.js & Python microservices\n- **Frontend**: React with streaming responses for a \"typing\" effect\n\n## Engineering Challenges\n\n### Handling Token Limits\nWe implemented a \"sliding window\" strategy and smart summarization to ensure we never hit context limits while keeping the most relevant information available to the model.\n\n### Latency Optimization\nTo achieve a \"feeling\" of instantaneity, we implemented **streaming responses**. The UI updates token-by-token as they are generated, reducing the perceived latency from ~3s to ~200ms.\n\n### Hallucination Control\nBy strictly constraining the model to \"Answer only using the provided context,\" we reduced hallucination rates significantly, ensuring users get accurate API references.\n\n## The Result\n\nWe moved beyond a simple chatbot to a **context-aware development partner**. This system reduced support ticket volume by **40%** and increased code completion rates for new users by **3x**. It turns the blank canvas problem into a conversation.","textTemplate":"","endCharIdx":3718,"metadataSeparator":"\n","type":"TEXT","hash":"K/G/FEpbysN/4FCuAO2kHzYaBhWw0LQq84yxc8xR4JY="},"7adc5313-056f-4d48-b577-1ac46a1e1fb7":{"id_":"7adc5313-056f-4d48-b577-1ac46a1e1fb7","metadata":{"source":"fitts-law-face-tracking.md","type":"blog","title":"A Fitts Law Evaluation of Two Type Face Tracking Inputs on Electronic Devices"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"62534f3e-45c1-4911-9d92-34ba122f836d","metadata":{"source":"fitts-law-face-tracking.md","type":"blog","title":"A Fitts Law Evaluation of Two Type Face Tracking Inputs on Electronic Devices"},"hash":"TP2rSJjQfAqldMvA9cWg2kLcUhIvQ+cRITI0utH3dWA="},"NEXT":{"nodeId":"7b0422a9-6240-46b7-83d1-5818512dbd1e","metadata":{},"hash":"lFat/HCEDxfsuUcGulf3uHiIZAklzY0Z3W5A/G6NBsA="}},"text":"## Overview\n\nThis study evaluates the usability and efficiency of two head-tracking input methodsâ€”**position-based** and **orientation-based**â€”for controlling electronic devices in a hands-free context. With the growing demand for intuitive, non-contact interactions, such as navigating smart devices while cooking, the research investigates how face-tracking can enable pointer control without external devices.\n\n![Fitts Law Study](/src/assets/images/projects/fitts/fitts2.jpg)\n\nInspired by **Fitts' Law**, the experiment focused on one-dimensional target acquisition tasks, comparing performance metrics like throughput, movement time, and error rate.\n\n## Research Methodology\n\nParticipants used a laptop with a built-in webcam to execute tasks under two conditions:\n1. **Head Position Tracking** - Movement based on head position in space\n2. **Head Orientation Tracking** - Movement based on head rotation/angle\n\n![Experimental Setup](/src/assets/images/projects/fitts/HCI_proj.001.jpeg)\n\n### Experimental Design\n\nThe study employed a controlled experimental setup to measure:\n- **Throughput** - Overall efficiency of the input method\n- **Movement Time** - Time taken to reach targets\n- **Error Rate** - Frequency of missed or incorrect selections\n- **User Comfort** - Subjective feedback on ease of use\n\n![Study Design](/src/assets/images/projects/fitts/HCI_proj.003.jpeg)\n\n## Key Findings\n\n### Position-Based Input Superiority\n\nResults indicate that **position-based input** yielded:\n- âœ… Higher throughput\n- âœ… Lower movement time\n- âœ… Better accuracy and efficiency\n- âœ… More intuitive user experience\n\n![Results Comparison](/src/assets/images/projects/fitts/HCI_proj.008.jpeg)\n\n### Challenges Identified\n\nHowever, several challenges impacted user performance:\n- **Tracking Reliability** - Variations in tracking accuracy\n- **Indoor Lighting Variability** - Environmental factors affecting detection\n- **User Fatigue** - Extended use causing discomfort\n- **Calibration Requirements** - Need for individual user calibration\n\n![Challenges Analysis](/src/assets/images/projects/fitts/HCI_proj.012.jpeg)\n\n## Technical Implementation\n\n### Technologies Used\n\n- **Computer Vision**: OpenCV for face detection and tracking\n- **Machine Learning**: Face landmark detection algorithms\n- **Web Technologies**: JavaScript for real-time processing\n- **Data Analysis**: Python for statistical analysis of results\n\n![Technical Architecture](/src/assets/images/projects/fitts/HCI_proj.006.jpeg)\n\n### Tracking Algorithms\n\nThe study implemented custom algorithms for:\n- Real-time face detection\n- Position and orientation calculation\n- Smoothing and filtering of tracking data\n- Adaptive calibration based on user behavior\n\n## Applications and Impact\n\n### Practical Applications\n\nThis research contributes to the design of accessible and practical hands-free interfaces for:\n\n1. **Smart Home Devices** - Control without touching surfaces\n2. **Cooking Assistance** - Navigate recipes while hands are busy\n3. **Accessibility Tools** - Support for users with limited mobility\n4. **Healthcare Settings** - Hygienic, contactless interactions\n5. **Industrial Applications** - Hands-free operation in manufacturing\n\n![Applications](/src/assets/images/projects/fitts/HCI_proj.015.jpeg)\n\n### Design Implications\n\nThe findings provide valuable insights for:\n- Interface designers creating hands-free systems\n- Developers implementing face-tracking features\n- Researchers studying alternative input methods\n- Product managers planning accessible technology\n\n## Future Research Directions\n\n### Potential Improvements\n\n1. **Enhanced Tracking Algorithms** - More robust detection in various lighting conditions\n2. **Hybrid Approaches** - Combining position and orientation for optimal performance\n3. **Adaptive Systems** - Learning user preferences and adjusting accordingly\n4. **Multi-Modal Input** - Integrating voice and gesture controls\n\n![Future Directions](/src/assets/images/projects/fitts/HCI_proj.017.jpeg)\n\n### Extended Studies\n\nFuture research could explore:\n- Long-term usability and user adaptation\n- Performance in real-world scenarios\n- Integration with other assistive technologies\n- Cross-cultural differences in interaction preferences\n\n## Conclusion\n\nThis study demonstrates that **position-based head tracking** offers superior performance compared to orientation-based methods for hands-free device control. While challenges remain in tracking reliability and environmental sensitivity, the findings pave the way for enhanced usability in diverse contexts.\n\n![Conclusion](/src/assets/images/projects/fitts/HCI_proj.018.jpeg)\n\nThe research contributes valuable insights to the field of Human-Computer Interaction, particularly in designing accessible, intuitive, and practical hands-free interfaces for smart devices.","textTemplate":"","endCharIdx":4807,"metadataSeparator":"\n","type":"TEXT","hash":"X6SessqI+mTXCIBaGb5mazbNalXzMgm9KYmbTLjahqA="},"7b0422a9-6240-46b7-83d1-5818512dbd1e":{"id_":"7b0422a9-6240-46b7-83d1-5818512dbd1e","metadata":{"source":"fitts-law-face-tracking.md","type":"blog","title":"A Fitts Law Evaluation of Two Type Face Tracking Inputs on Electronic Devices"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"62534f3e-45c1-4911-9d92-34ba122f836d","metadata":{"source":"fitts-law-face-tracking.md","type":"blog","title":"A Fitts Law Evaluation of Two Type Face Tracking Inputs on Electronic Devices"},"hash":"TP2rSJjQfAqldMvA9cWg2kLcUhIvQ+cRITI0utH3dWA="},"PREVIOUS":{"nodeId":"7adc5313-056f-4d48-b577-1ac46a1e1fb7","metadata":{"source":"fitts-law-face-tracking.md","type":"blog","title":"A Fitts Law Evaluation of Two Type Face Tracking Inputs on Electronic Devices"},"hash":"kjmGNLOIkxMT/Q7l4N6U6toosQwDy6gSLisWNiSgWDk="}},"text":"**Adaptive Systems** - Learning user preferences and adjusting accordingly\n4. **Multi-Modal Input** - Integrating voice and gesture controls\n\n![Future Directions](/src/assets/images/projects/fitts/HCI_proj.017.jpeg)\n\n### Extended Studies\n\nFuture research could explore:\n- Long-term usability and user adaptation\n- Performance in real-world scenarios\n- Integration with other assistive technologies\n- Cross-cultural differences in interaction preferences\n\n## Conclusion\n\nThis study demonstrates that **position-based head tracking** offers superior performance compared to orientation-based methods for hands-free device control. While challenges remain in tracking reliability and environmental sensitivity, the findings pave the way for enhanced usability in diverse contexts.\n\n![Conclusion](/src/assets/images/projects/fitts/HCI_proj.018.jpeg)\n\nThe research contributes valuable insights to the field of Human-Computer Interaction, particularly in designing accessible, intuitive, and practical hands-free interfaces for smart devices.\n\n## Research Team\n\n- **Temirlan Dzhoroev** - Software Engineer & Researcher\n- **Joe, BH Kim** - Software Engineer & Researcher\n\n## Publication\n\nThis research was presented at **HCI Korea 2022**:\n\n> Dzhoroev, T., Kim, B.H., & Lee, H.S. (2022). Comparison of Face Tracking and Eye Tracking for Scrolling a Web Browser on Mobile Devices. _HCI Korea_, pp. 227â€“231.","textTemplate":"","startCharIdx":3770,"endCharIdx":5168,"metadataSeparator":"\n","type":"TEXT","hash":"1+5Xxn6bg7Z6fy4asaTpSACesRGOtvOlIIazp4NCLxY="},"1570cfa6-12f4-4937-8194-1217d63a8201":{"id_":"1570cfa6-12f4-4937-8194-1217d63a8201","metadata":{"source":"publications.md","type":"blog","title":"Publications"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"a9795a7e-4958-47bf-8425-88d8afe8b126","metadata":{"source":"publications.md","type":"blog","title":"Publications"},"hash":"IM+jEmVnZSoUMQ+N+kprVxNAyITsPeXAL+3NUO7Hb9I="}},"text":"## Journal\n\n- Park, H., Lee, J., **Dzhoroev, T.**, Kim, B., & Lee, H. S. (2024). Developing a Dynamic Expression Model That Can Simultaneously Control Robot's Facial and Movement Expressions. _Journal of Institute of Control, Robotics and Systems_, 30(1), 8â€“12\n\n- Lee, J., Park, H., **Dzhoroev, T.**, Kim, B., & Lee, H. S. (2023). The Implementation and Analysis of Facial Expression Customization for a Social Robot. _The Journal of Korea Robotics Society_, 18(2), 203â€“215\n\n- Park, H., Lee, J., **Dzhoroev, T.**, Kim, B., & Lee, H. S. (2023). Expanded Linear Dynamic Affect-Expression Model for Lingering Emotional Expression in Social Robots. _Intelligent Service Robotics_, 16(5), 619â€“631\n\n- Jang, S.S., Lee, S.H., **Dzhoroev, T.**, Kim, T.Y., Oh, H.J., Kim, N.R., & Park, Y-W. (2021). Design Guidelines for Contextual Awareness and Management of Hygiene in Daily Life with Infectious Viruses. _Archives of Design Research_, 34(3), 101â€“121\n\n## Conference\n\n- **Dzhoroev, T.**, Park, H., Lee, J., Kim, B., & Lee, H. S. (2023). Human Perception on Social Robot's Face and Color Expression Using Computational Emotion Model. _IEEE RO-MAN 2023_, pp. 2484â€“2491\n\n- **Dzhoroev, T.**, Park, S.Y., Park, H.E., Lee, J.Y., & Lee, H.S. (2022). An Expressive Eye Interface for Pedestrian Interaction with Indoor Mobility. _ICROS_, pp. 267â€“268\n\n- Park, S.Y., **Dzhoroev, T.**, Yoon, S.H., & Lee, H.S. (2022). Driving Performance Improvement and Recognition Algorithm Development of a Pedestrian for Indoor Shared Mobility (Korean). _ICROS_, pp. 400â€“401\n\n- **Dzhoroev, T.**, Kim, B.H., & Lee, H.S. (2022). Comparison of Face Tracking and Eye Tracking for Scrolling a Web Browser on Mobile Devices. _HCI Korea_, pp. 227â€“231\n\n- Kim, N.R., Lee, S.H., Oh, H.J., **Dzhoroev, T.**, & Park, Y-W. (2021). Interactive System Design in Everyday Life to Improve the Perception of Environmental Hygiene Information in Pandemic Situations (Korean). _KSDS_, pp. 256â€“257\n\n- Lee, K-R., Ju, S., **Dzhoroev, T.**, Goh, G., Lee, M-H., & Park, Y-W. (2020). DayClo: An Everyday Table Clock Providing Interaction with Personal Schedule Data for Self-reflection. _DIS'20_, pp. 1793â€“1806","textTemplate":"","endCharIdx":2150,"metadataSeparator":"\n","type":"TEXT","hash":"IlbNjQkQWKR27jsW9UXr9oaxFo9qEVsRpYtBtFrwWYM="},"f7481dea-b9cd-4a2e-9f6f-dbb156504867":{"id_":"f7481dea-b9cd-4a2e-9f6f-dbb156504867","metadata":{"source":"redbrick-frontend-ux-engineer.md","type":"blog","title":"Architecting a Web-based 3D Game Engine"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"1ab4a2de-40b5-4f30-b6a7-4ff7b3ddb2e4","metadata":{"source":"redbrick-frontend-ux-engineer.md","type":"blog","title":"Architecting a Web-based 3D Game Engine"},"hash":"oADYuUhMq8XFLy12b1WXGElBIZqndllbFKTRP3o/mCY="}},"text":"## The Engineering Challenge\n\nBuilding a 3D game engine that runs entirely in the browser is a battle against performance constraints. Unlike native engines (Unity/Unreal), we don't have direct hardware access. We have to manage memory, garbage collection, and the DOMâ€”all while maintaining a steady 60 FPS.\n\nAt Redbrick, I led the frontend engineering efforts to modernize our 3D engine, focusing on **rendering optimization**, **scalable UI architecture**, and **cross-platform compatibility**.\n\n![Redbrick Engine Architecture](/src/assets/images/projects/redbrick/cover-02.png)\n\n## Core Architecture\n\nThe engine is built on a hybrid stack:\n- **Core Renderer**: Three.js with custom WebGL shaders for performance-critical visual effects.\n- **State Management**: Redux + RxJS for handling complex game states and event streams.\n- **UI Layer**: React for the editor interface, decoupled from the render loop to prevent UI updates from causing frame drops.\n\n### ðŸš€ Performance Optimization\n\nOne of our biggest hurdles was **draw calls**. User-generated content is unoptimized by nature. To handle scenes with thousands of objects, I implemented:\n\n1.  **Instanced Mesh Rendering**: Automatically grouping identical geometries (like trees or coins) into a single draw call, reducing GPU overhead by **90%**.\n2.  **Level of Detail (LOD) Systems**: Dynamically swapping high-poly models for low-poly versions based on camera distance.\n3.  **Texture Atlasing**: Merging multiple textures into a single atlas at runtime to minimize context switching.\n\n## Feature Deep Dives\n\n### ðŸ¥½ WebXR Integration\nWe wanted to bridge the gap between web and VR. I integrated the **WebXR Device API**, allowing any game created on our platform to be instantly playable on Meta Quest and other VR headsets without a separate build step.\n\n- Implemented a custom **Input Manager** that abstracts controller inputs (Touch, Mouse, VR Controllers) into a unified event system.\n- Built a **VR Camera Rig** that handles teleportation and room-scale tracking automatically.\n\n### ðŸ§© Visual Scripting System\nTo democratize game creation, we built a block-based coding environment (Blockly) that compiles down to JavaScript.\n\n- **Challenge**: Running user code safely.\n- **Solution**: We implemented a sandboxed execution environment using Web Workers, ensuring that an infinite loop in a user's script doesn't freeze the main UI thread.\n\n![Visual Scripting Interface](/src/assets/images/projects/redbrick/cover-04.png)\n\n## Unity WebGL Bridge\n\nWe needed to support professional developers using Unity. I built a two-way communication bridge between the Unity WebGL build and our React frontend.\n\n```typescript\n// Example: React <-> Unity Bridge\nclass UnityBridge {\n  sendMessage(method: string, value: any) {\n    this.unityInstance.SendMessage('GameManager', method, value);\n  }\n\n  // Listening for events from Unity\n  onGameEvent(event: GameEvent) {\n    store.dispatch(updateGameState(event));\n  }\n}\n```\n\nThis allowed us to wrap Unity games with our platform's UI, authentication, and monetization systems seamlessly.\n\n## Impact & Scale\n\n- **100,000+** Games created and played.\n- **50%** Reduction in load times through asset bundling and lazy loading.\n- **Zero** build steps required for users to publish cross-platform (Mobile, Desktop, VR).\n\nThis project pushed the boundaries of what's possible in a browser, proving that \"Web 3.0\" isn't just a buzzwordâ€”it's a viable platform for high-fidelity interactive experiences.","textTemplate":"","endCharIdx":3491,"metadataSeparator":"\n","type":"TEXT","hash":"HIon/dNvn03LUjsyE0sAlT5B6D0Vw9/OK0I0xQHE+FE="},"dab09d58-11e8-4774-aedd-41c89d546e7a":{"id_":"dab09d58-11e8-4774-aedd-41c89d546e7a","metadata":{"source":"CV_temirlan_dzhoroev.pdf","type":"pdf","title":"CV temirlan dzhoroev"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"abf4b287-bf6b-481c-9526-4fdf9a2cd5ee","metadata":{"source":"CV_temirlan_dzhoroev.pdf","type":"pdf","title":"CV temirlan dzhoroev"},"hash":"TDLMYqKSSBsx+P2Vk6Yts0Y1bkFJby2lQ+IxOxK7B3I="},"NEXT":{"nodeId":"5025e6a4-4a7e-4c9b-9541-8d41ece89b6e","metadata":{},"hash":"nIVgOEpydzbUzxg3UQ6zeWXHsQZXWtyDLPjC6eJqYjk="}},"text":"Temirlan Dzhoroev + Seoul, South Korea # dzhoroev1@gmail.com \" +82-10-7393-2412 \u0012 mastertim.xyz Ã° dzhoroev7 Â§ master-tim Â§ dev-tima Experience Redbrick Inc. Full-stack AI Engineer & Technical PM Seoul, South Korea Jul 2024 â€“ present â—¦ Built an AI game creation system from the scratch, enabling prompt-based content generation, which contributed to securing $ 1.2M R&D investment in Korea . â—¦ Developed multiple AI agents with diverse creative and functional purposes. â—¦ Optimized code generation pipelines, reducing token usage by 80% . â—¦ Designed and enhanced save, update, and remix features for seamless content iteration. â—¦ Implemented ad integration , allowing creators to monetize games and metaverses effortlessly. â—¦ Integrated Unity WebGL support , expanding platform compatibility with Unity applications. â—¦ Delivered a new, modern UI framework , improving overall UX and design consistency. â—¦ Migrated core packages to modern libraries, boosting scalability and performance. 3D Engine Engineer (Web 3D Engine) Jul 2023 â€“ Jul 2024 â—¦ Refactored legacy code, improving scalability and maintainability. â—¦ Built and maintained a web-based 3D Studio Engine with a clean, modern UI. â—¦ Integrated Blockly , enabling no-code/low-code content creation. â—¦ Implemented WebXR support , unlocking AR/VR development in the browser. â—¦ Added AI code assistants , helping creators generate and debug scripts faster. â—¦ Partnered with external AI companies to integrate text-to-model and text-to-avatar (3D) features. â—¦ Improved onboarding by addressing new user pain points , boosting adoption and usability. â—¦ Shipped 10+ online 3D web games , reaching over 1M+ total plays . UNIST â€” DECS Lab. Research assistant & Embedded systems engineer Ulsan, South Korea Mar 2021 â€“ Mar 2023 â—¦ Designed and developed expressive robotic faces to enhance HRI & HCI. â—¦ Programmed and debugged embedded systems for interaction research. â—¦ Authored 6 conference papers, 4 journal articles, and co-filed 6 patents. Education UNIST â€” Ulsan National Institute of Science and Technology MS in Design (Human Computer Interaction) Feb 2023 â—¦ Lotte Scholarship UNIST â€” Ulsan National Institute of Science and Technology BS in Computer Science and Industrial Design Feb 2021 â—¦ Global UNISTAR Silver Scholarship Korea University Korean Language and Literature (Intermediate level) Aug 2018 â€“ Feb 2019 Skills Technical: JavaScript, TypeScript, Python, C++, React, Next.js, Three.js, WebGL, Node.js, REST APIs, RAG integration, AI agents, text-to-model, text-to-avatar, ML pipelines, LangChain, vector databases, LLM fine-tuning, embeddings Soft: Product management, UX/UI design, creative, problem solving, cross-team collaboration, multilingual (English, Korean, Russian) Temirlan Dzhoroev - Page 1 of 2 Last updated in September 2024 \n\nPublications Developing a Dynamic Expression Model That Can Simultaneously Con- trol Robotâ€™s Facial and Movement Expressions Park, H., Lee, J., Dzhoroev, T. , Kim, B., & Lee, H. S. Journal of Institute of Control, Robotics and Systems , 30(1), 8â€“12 2024 The Implementation and Analysis of Facial Expression Customization for a Social Robot Lee, J., Park, H., Dzhoroev, T. , Kim, B., & Lee, H. S. The Journal of Korea Robotics Society , 18(2), 203â€“215 2023 Human Perception on Social Robotâ€™s Face and Color Expression Using Computational Emotion Model Dzhoroev, T. , Park, H., Lee, J., Kim, B., & Lee, H. S. IEEE RO-MAN 2023 , pp. 2484â€“2491 2023 Expanded Linear Dynamic Affect-Expression Model for Lingering Emo- tional Expression in Social Robots Park, H., Lee, J., Dzhoroev, T. , Kim, B., & Lee, H. S. Intelligent Service Robotics , 16(5), 619â€“631 2023 An Expressive Eye Interface for Pedestrian Interaction with Indoor Mo- bility Dzhoroev, T. , Park, S.Y., Park, H.E., Lee, J.Y., & Lee, H.S. ICROS , pp.","textTemplate":"","endCharIdx":3809,"metadataSeparator":"\n","type":"TEXT","hash":"5BfvzZTZZOtjll/cz1qVzTfLzz/jJs3Sv9p1Qt5HKEA="},"5025e6a4-4a7e-4c9b-9541-8d41ece89b6e":{"id_":"5025e6a4-4a7e-4c9b-9541-8d41ece89b6e","metadata":{"source":"CV_temirlan_dzhoroev.pdf","type":"pdf","title":"CV temirlan dzhoroev"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"abf4b287-bf6b-481c-9526-4fdf9a2cd5ee","metadata":{"source":"CV_temirlan_dzhoroev.pdf","type":"pdf","title":"CV temirlan dzhoroev"},"hash":"TDLMYqKSSBsx+P2Vk6Yts0Y1bkFJby2lQ+IxOxK7B3I="},"PREVIOUS":{"nodeId":"dab09d58-11e8-4774-aedd-41c89d546e7a","metadata":{"source":"CV_temirlan_dzhoroev.pdf","type":"pdf","title":"CV temirlan dzhoroev"},"hash":"caHPu+WwIVrc/ul1HkH/zwrTlBjDOJybghvn1aTmHUQ="}},"text":", Park, H., Lee, J., Kim, B., & Lee, H. S. IEEE RO-MAN 2023 , pp. 2484â€“2491 2023 Expanded Linear Dynamic Affect-Expression Model for Lingering Emo- tional Expression in Social Robots Park, H., Lee, J., Dzhoroev, T. , Kim, B., & Lee, H. S. Intelligent Service Robotics , 16(5), 619â€“631 2023 An Expressive Eye Interface for Pedestrian Interaction with Indoor Mo- bility Dzhoroev, T. , Park, S.Y., Park, H.E., Lee, J.Y., & Lee, H.S. ICROS , pp. 267â€“268 2022 Driving Performance Improvement and Recognition Algorithm Develop- ment of a Pedestrian for Indoor Shared Mobility (Korean) Park, S.Y., Dzhoroev, T. , Yoon, S.H., & Lee, H.S. ICROS , pp. 400â€“401 2022 Comparison of Face Tracking and Eye Tracking for Scrolling a Web Browser on Mobile Devices Dzhoroev, T. , Kim, B.H., & Lee, H.S. HCI Korea , pp. 227â€“231 2022 Design Guidelines for Contextual Awareness and Management of Hy- giene in Daily Life with Infectious Viruses Jang, S.S., Lee, S.H., Dzhoroev, T. , Kim, T.Y., Oh, H.J., Kim, N.R., & Park, Y-W. Archives of Design Research , 34(3), 101â€“121 2021 Interactive System Design in Everyday Life to Improve the Perception of Environmental Hygiene Information in Pandemic Situations (Korean) Kim, N.R., Lee, S.H., Oh, H.J., Dzhoroev, T. , & Park, Y-W. KSDS , pp. 256â€“257 2021 DayClo: An Everyday Table Clock Providing Interaction with Personal Schedule Data for Self-reflection Lee, K-R., Ju, S., Dzhoroev, T. , Goh, G., Lee, M-H., & Park, Y-W. DISâ€™20 , pp. 1793â€“1806 2020 Certifications & Awards Advanced React Meta Principles of UX/UI Design Meta Three.js Journey Three.js Journey Algorithmic Toolbox UC San Diego Korean Language Korea Univercity First degree diploma in Physics National Olympiad Ministry of Education of Kyrgyzstan Temirlan Dzhoroev - Page 2 of 2","textTemplate":"","startCharIdx":3368,"endCharIdx":5135,"metadataSeparator":"\n","type":"TEXT","hash":"ehG6GWgolUtplVSMCoXc0C31IwI9whKpmgg9wX8tpB4="},"63606d87-b455-478d-a337-8b81dc3596a6":{"id_":"63606d87-b455-478d-a337-8b81dc3596a6","metadata":{"source":"general_background.txt","type":"text_doc","title":"general background"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"14e16a6f-93bc-4457-acae-0cf87e1749cc","metadata":{"source":"general_background.txt","type":"text_doc","title":"general background"},"hash":"XsMhQKEtw507WENKj2HjZn4Bg89mEOo6hR0ZFCcvnVU="},"NEXT":{"nodeId":"c0bc2a94-8c47-4638-8cf1-5781c9727237","metadata":{},"hash":"6DOVbZA5PQTSD+bw35SleJryQkPBY8AIejnt5fOX0rY="}},"text":"# General Professional Background & Motivation\n\n## Motivation\nMy motivation came organically from my background â€” with a BSc in Computer Science and Industrial Design and an MSc in Design focusing on HCI and HRI, Iâ€™ve always been driven to create highly interactive, human-centered systems. At Redbrick, I saw an opportunity to combine strong front-end engineering with AI integration, enabling new forms of interactive content such as AI-powered game creation and immersive 3D environments.\n\nOn a personal level, strengthening my front-end expertise while layering in AI integration has given me the confidence to lead technical direction on highly interactive products. On the team level, these skills have translated into measurable impact â€” for example, the AI-powered game creation system I helped architect contributed to securing $1.2M in R&D investment, while optimization of our AI pipelines reduced token usage by 80%. My front-end work on modernizing UI frameworks and integrating Unity WebGL support has also improved scalability, maintainability, and user experience across the platform.\n\nI have extensive experience with JavaScript and TypeScript as my primary programming languages for web development, using frameworks such as React, Next.js, and Three.js/WebGL. Iâ€™m highly proficient in React and CSS, particularly in designing responsive, performant interfaces that deliver excellent user experiences across complex products. While my core focus is front-end engineering, I also have strong experience in Python, which Iâ€™ve applied in building AI integration pipelines, ML workflows, and backend services.\n\nAt Redbrick, Iâ€™ve built large systems that span multiple services â€” for example, in our AI game creation platform I integrated web front ends, REST APIs, vector databases, and AI pipelines. I also worked on scaling our 3D Studio Engine, ensuring seamless connections between the front end, API services, asset storage, and real-time event processing.\n\nFor me, the key to building large systems that are reliable and maintainable comes down to three principles: clean and modular code, good documentation, and effective tracking systems such as Jira and GitHub.\n\nWhile I donâ€™t yet have direct experience with Flutter, given my front-end background with React and Next.js, Iâ€™m confident I could learn and apply it quickly. Many of the principles I already use â€” clean component architecture, state management, and responsive UI design â€” translate directly.\n\n## Software Engineering Experience\nMost of my professional work has been at Redbrick, where Iâ€™ve contributed to large-scale, user-facing projects. I helped build the 3D Studio Engine â€” a web-based interactive environment supporting 3D creation, WebGL/Three.js rendering, and AR/VR experiences. More recently, Iâ€™ve led development of an AI-to-game content generation system, where creators can use prompts to generate playable games, remix content, and integrate monetization. Alongside these, Iâ€™ve delivered modern UI frameworks, REST API integrations, and optimized AI pipelines to improve scalability and efficiency.\n\nEarlier in my career, I also worked on embedded systems and robotics. For example, I developed an autonomous robot capable of SLAM-based localization, navigation, and object manipulation, and designed expressive robotic faces with web/socket interfaces. I also mentored design students in coding and hardware optimization using Arduino and Raspberry Pi, helping them turn creative concepts into working prototypes.\n\nMy core languages are JavaScript, TypeScript, Python, and C++. I work primarily on Linux and macOS, and rely on environments such as VS Code, GitHub, and Jira for collaboration. On the AI side, Iâ€™ve worked with REST APIs, vector databases, LangChain, and RAG pipelines.\n\nAt Redbrick, Iâ€™ve led development of products shipped to thousands of users. I guided the technical direction of the AI game creation system â€” from prompt-to-game generation to Unity WebGL integration â€” and delivered modern UI frameworks and new features for the 3D Studio Engine, which reached over 1M plays across 10+ games. In both cases, I worked as both Full-stack Engineer and Technical PM, responsible for technical architecture, roadmap planning, cross-team collaboration, and release delivery.\n\nMy most senior role is my current one at Redbrick, where I serve as Full-stack AI Engineer & Technical PM. Iâ€™ve led small cross-functional teams of engineers, game developers, and designers, setting technical direction while aligning creative and engineering goals.\n\nFrom my experience, the role of an engineering manager is to create the conditions where engineers can do their best work: setting clear technical direction, breaking down problems into achievable tasks, and fostering collaboration across disciplines.\n\nI view performance as something built in from the start: clean and modular code, React and WebGL optimizations (minimizing re-renders, efficient state management, smaller bundles), and optimized AI pipelines (e.g., 80% token reduction). Quality comes from discipline â€” clean code, code reviews, testing, and good tracking systems like Jira and GitHub to manage bugs and technical debt.","textTemplate":"","endCharIdx":5197,"metadataSeparator":"\n","type":"TEXT","hash":"SOQNLHOGGL8+unQszpk7kypt5anL2bA4LgcOyL8bGkk="},"c0bc2a94-8c47-4638-8cf1-5781c9727237":{"id_":"c0bc2a94-8c47-4638-8cf1-5781c9727237","metadata":{"source":"general_background.txt","type":"text_doc","title":"general background"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"14e16a6f-93bc-4457-acae-0cf87e1749cc","metadata":{"source":"general_background.txt","type":"text_doc","title":"general background"},"hash":"XsMhQKEtw507WENKj2HjZn4Bg89mEOo6hR0ZFCcvnVU="},"PREVIOUS":{"nodeId":"63606d87-b455-478d-a337-8b81dc3596a6","metadata":{"source":"general_background.txt","type":"text_doc","title":"general background"},"hash":"Kn3hWd4xSDs0oh5Dt+ufSkpIHkLYjDjnDuPucahghBg="}},"text":"My most senior role is my current one at Redbrick, where I serve as Full-stack AI Engineer & Technical PM. Iâ€™ve led small cross-functional teams of engineers, game developers, and designers, setting technical direction while aligning creative and engineering goals.\n\nFrom my experience, the role of an engineering manager is to create the conditions where engineers can do their best work: setting clear technical direction, breaking down problems into achievable tasks, and fostering collaboration across disciplines.\n\nI view performance as something built in from the start: clean and modular code, React and WebGL optimizations (minimizing re-renders, efficient state management, smaller bundles), and optimized AI pipelines (e.g., 80% token reduction). Quality comes from discipline â€” clean code, code reviews, testing, and good tracking systems like Jira and GitHub to manage bugs and technical debt.\n\n## Education\nIn high school, I achieved mostly top grades (all 5s, equivalent to A, with only two 4s) and earned a First Degree Diploma in the National Physics Olympiad, placing me among the top 1% of students nationally. In language, I was an A student, though my primary focus was on mathematics and physics.\n\nFor my undergraduate and graduate studies, I chose UNIST in South Korea, earning a BSc in Computer Science and Industrial Design and later an MSc in Design (HCI/HRI). I had considered Kyoto University, but opted for an English-language environment. My BSc GPA was about 87.2%, with particularly strong results in CS and Industrial Design. In my MSc, I achieved 97% overall, supported by a Lotte Scholarship. I also co-authored 6 conference papers, 4 journal articles, and co-filed 6 patents during my studies.\n\nA key leadership role was mentoring design students in coding and prototyping with Arduino and Raspberry Pi, helping them translate creative concepts into functional prototypes.\n\n## Professional Outlook\nWhat excites me most about software engineering is the chance to combine front-end expertise with impactful missions. Having built complex systems like AI game creation and the 3D Studio Engine, I know the impact that well-designed, performant, and accessible interfaces can have. The opportunity to contribute to open-source toolkits and help shape consistent user experiences globally is deeply motivating. Iâ€™m also eager to grow in new technologies like Flutter, which I see as a promising step toward unifying desktop and web development. Above all, I want to be at the intersection of web technologies, open-source collaboration, and large-scale global impact.","textTemplate":"","startCharIdx":4292,"endCharIdx":6890,"metadataSeparator":"\n","type":"TEXT","hash":"Dn/Cq0Q/z59B1UkBR3jU+C5OqmGF5atMcPcdwCviYNQ="}},"type":"simple_dict"}}}